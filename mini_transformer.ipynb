{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQFBvsY+WXP/EshTOagMBs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanyag/tiny_llm_colab/blob/main/mini_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "SnLLXikWBRE8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read File"
      ],
      "metadata": {
        "id": "EpIFA6cIBoFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/sample_data/sample_text.txt', 'r') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "iV2ocbY9BnLU"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ky6ETdVvB5ab"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizer"
      ],
      "metadata": {
        "id": "kFFlQa7pCaIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for ch, i in stoi.items()}\n",
        "\n",
        "def encode(s): return [stoi[c] for c in s]\n",
        "def decode(t): return ''.join([itos[i] for i in t])\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "Uvdc0os7CfkO"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß™ 4. Train/Validation Split\n"
      ],
      "metadata": {
        "id": "b2vBDJgpCi_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n"
      ],
      "metadata": {
        "id": "q7MNZbuiCxj_"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚öôÔ∏è 5. Hyperparameters\n"
      ],
      "metadata": {
        "id": "np3oA1CEDDrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 64   # max context length\n",
        "batch_size = 32\n",
        "n_embed = 64\n",
        "n_head = 4\n",
        "n_layer = 2\n",
        "dropout = 0.1\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
      ],
      "metadata": {
        "id": "PLf4TOLLDEcz"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Step 2: Define the Transformer Model (in Colab)"
      ],
      "metadata": {
        "id": "L37EwLhJDKVr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ 1. Embedding + Positional Encoding"
      ],
      "metadata": {
        "id": "VJk9Y4KCDcgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, n_embed):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding = nn.Embedding(block_size, n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T = x.shape\n",
        "        token_emb = self.token_embedding(x)               # (B, T, n_embed)\n",
        "        pos_emb = self.position_embedding(torch.arange(T, device=x.device))  # (T, n_embed)\n",
        "        return token_emb + pos_emb  # (B, T, n_embed)\n"
      ],
      "metadata": {
        "id": "yiAGbfu_DfR_"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ 2. Single Head of Self-Attention"
      ],
      "metadata": {
        "id": "N9ICvuNLDreJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionHead(nn.Module):\n",
        "    def __init__(self, n_embed, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # causal mask: prevents attending to future\n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)   # (B, T, head_size)\n",
        "        q = self.query(x) # (B, T, head_size)\n",
        "\n",
        "        wei = q @ k.transpose(-2, -1) * (C ** -0.5)  # scaled dot-product\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        v = self.value(x)\n",
        "        out = wei @ v  # (B, T, head_size)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "kf3m3Tk4DuJA"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ 3. Multi-Head Attention"
      ],
      "metadata": {
        "id": "mryExUH3D7od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_embed, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head\n",
        "        self.heads = nn.ModuleList([SelfAttentionHead(n_embed, head_size) for _ in range(n_head)])\n",
        "        self.proj = nn.Linear(n_embed, n_embed)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "0veAC5JZD-8L"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ 4. FeedForward Network"
      ],
      "metadata": {
        "id": "m9t12ztKEFZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4 * n_embed),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embed, n_embed),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "HySIu7arEJTv"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ 5. Transformer Block (Attention + FeedForward)"
      ],
      "metadata": {
        "id": "YsGIc9PQEQm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, n_embed, n_head):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "        self.attn = MultiHeadAttention(n_embed, n_head)\n",
        "        self.ffwd = FeedForward(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "Is9zgILaEUhb"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ 6. Full Transformer Model"
      ],
      "metadata": {
        "id": "MB3JVNqvEYHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TinyTransformer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed = TokenEmbedding(vocab_size, n_embed)\n",
        "        self.blocks = nn.Sequential(*[TransformerBlock(n_embed, n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embed)\n",
        "        self.head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        x = self.embed(idx)            # (B, T, n_embed)\n",
        "        x = self.blocks(x)             # transformer layers\n",
        "        x = self.ln_f(x)               # final layer norm\n",
        "        logits = self.head(x)          # (B, T, vocab_size)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "fOxQeDv-EaYo"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Train the Transformer and Generate Text in Colab."
      ],
      "metadata": {
        "id": "K1a3ndQ0EkL9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Step 3A: Create Batches"
      ],
      "metadata": {
        "id": "FjqWf3SVElUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "    data_split = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data_split) - block_size, (batch_size,))\n",
        "    x = torch.stack([data_split[i:i + block_size] for i in ix])\n",
        "    y = torch.stack([data_split[i + 1:i + 1 + block_size] for i in ix])\n",
        "    return x.to(device), y.to(device)\n"
      ],
      "metadata": {
        "id": "HSwVEosYElAn"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Step 3B: Instantiate the Model and Optimizer"
      ],
      "metadata": {
        "id": "P04zWOMIEs8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = TinyTransformer().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "rjS_GEwbEu40"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Step 3C: Training Loop\n",
        "\n"
      ],
      "metadata": {
        "id": "rol6S9gGE1kp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_iters = 2000\n",
        "eval_interval = 200\n",
        "\n",
        "for step in range(max_iters):\n",
        "    if step % eval_interval == 0:\n",
        "        model.eval()\n",
        "        xb, yb = get_batch('val')\n",
        "        with torch.no_grad():\n",
        "            logits = model(xb)\n",
        "            loss = F.cross_entropy(logits.view(-1, vocab_size), yb.view(-1))\n",
        "        print(f\"Step {step}: val loss = {loss.item():.4f}\")\n",
        "        model.train()\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits = model(xb)\n",
        "    loss = F.cross_entropy(logits.view(-1, vocab_size), yb.view(-1))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVeQzNQdE2U1",
        "outputId": "77d2541a-369d-4911-9a73-63b73da28da5"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: val loss = 3.9451\n",
            "Step 200: val loss = 2.2010\n",
            "Step 400: val loss = 2.1170\n",
            "Step 600: val loss = 2.0421\n",
            "Step 800: val loss = 2.1522\n",
            "Step 1000: val loss = 2.2564\n",
            "Step 1200: val loss = 2.4154\n",
            "Step 1400: val loss = 2.4909\n",
            "Step 1600: val loss = 2.8642\n",
            "Step 1800: val loss = 2.5145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Step 3D: Generate Text from the Model"
      ],
      "metadata": {
        "id": "Y9UVZGXtE4n9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def generate(model, start_text, length=200):\n",
        "    model.eval()\n",
        "    context = torch.tensor([[stoi[c] for c in start_text]], dtype=torch.long).to(device)\n",
        "    for _ in range(length):\n",
        "        context_condensed = context[:, -block_size:]\n",
        "        logits = model(context_condensed)\n",
        "        probs = F.softmax(logits[:, -1, :], dim=-1)\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "        context = torch.cat((context, next_token), dim=1)\n",
        "    return decode(context[0].tolist())\n"
      ],
      "metadata": {
        "id": "0ZBM9QpVE8Z8"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß™ Test It:"
      ],
      "metadata": {
        "id": "zkH-DgdkFX47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate(model, start_text=\"After growing up\", length=300))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlJjtQPdFbpS",
        "outputId": "895650b5-28ba-4c94-f82b-d97353ca49af"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After growing up, and there was no more chocolate milk at bedtime, norg a where chapter of books read a oudd, a the ry her very nigh made up new stories for herself, and she liked boxes full of things, and she liked boxes full of things, and she liked to know how and where to find things read again. But every night\n"
          ]
        }
      ]
    }
  ]
}